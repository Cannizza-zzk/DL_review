#### 定义：

​	信息论中，熵的定义为：**无损编码事件信息的最小平均编码长度。**

#### 直观解释：

​	熵较大，则编码长度较长，意味着信息的可能状态较多，相应的每一状态的概率较小。 

#### 熵的计算：

​	假设一个信息事件有8种可能的状态，且各状态等可能性，即可能性都是12.5%=1/8，则需3位二进制数来进行编码。扩展到一般情况，对于具有N种等可能状态的信息，每种状态的可能性P = 1/N，编码该信息所需的最小编码长度为：

​																			$log(N) = -log(1/N) = -log(P)$ 

​	则熵的公式为：

​																				$Entropy = - \Sigma P_i *log(P_i)$     

​														*平均最小编码长度 = min（事件概率 \* 事件编码长度）*

​	从个状态等可能的情况推广到各状态不等可能的情况，目前我个人的理解是，假如某信息状态的可能性是$k/N$的话，可以将整个信息事件看作是由$N/k$ 个等可能事件构成的，这样该信息状态最小编码长度为$log(N/k) = -log(k/N)$。信息事件的熵即回归到上式。基本的思路有点像IP地址划分，严谨性应该是有点欠缺，可以查证一下有关信息论的书籍。 

​	连续变量x的熵公式：

​																				$Entropy = -\int P_i * log(P_i)$ 

​	因此，熵可以看作是$-log(P_i)$的期望，对于符合概率分布P的随机变量x（$x \sim P$），信息熵可写作：

​																	$H(P) = Entropy = E_{x \sim P}[-log(P(x))]$ 

#### 交叉熵：

​	熵是服从某一特定概率分布事件的理论最小平均编码长度，可以认为，知道事件的概率分布就可以计算出它的熵。在不知道真实概率分布P的情况下，我们只有预估的概率分布Q，使用估计得到的概率分布，可以计算估计的熵：

​																	$EstimatedEntropy = E_{x \sim Q}[-log(Q(x))]$ 

​	估计的概率分布为我们的公式引入了两部分的不确定性：

* 计算期望的概率分布是Q，与真实的概率分布P不同。

- 计算最小编码长度的概率是 -logQ，与真实的最小编码长度 -logP 不同。


​	

​	假设经过观测后，我们得到了真实概率分布P，就可以使用P计算平均编码长度，实际编码长度基于Q计算，这个计算结果就是P和Q的交叉熵。这样，实际编码长度和理论最小编码长度就有了对比的意义。

​																$H(P,Q) = CrossEntropy = E_{x \sim P}[-log(Q(x))]$

​	交叉熵使用H(P,Q)表示，意味着使用P计算期望，使用Q计算编码长度；所以H(P,Q)并不一定等于H(Q,P)，除了在P=Q的情况下，H(P,Q) = H(Q,P) = H(P)。

​	对于期望，我们使用真实概率分布P来计算；对于编码长度，我们使用假设的概率分布Q来计算，因为它是预估用于编码信息的。因为熵是理论上的平均最小编码长度，所以交叉熵只可能大于等于熵。*个人理解，交叉熵是用于衡量估计的概率分布Q和实际概率分布Q的差距的，因此H(P)应该就是label。*

	#### 交叉熵作为损失函数：

​	上文所述，交叉熵主要用来衡量两个概率分布的相似程度。

​	对于多分类问题，假设有C个类别，则标签**y** 通常是一个C维的one-hot向量。可以视作样本标签的真实条件概率分布$P(y|x)$ .模型的预测结果$\widetilde{y}$ 同样是一个C维向量，可以视作是预估分布Q。衡量预测结果的有效性，就可以转变为对两个概率分布相似性的分析，因此交叉熵可以用作多分类模型的损失函数。

​	二分类问题中交叉熵可以写成：

​													 $BinaryCrossEntropy = -Plog(\widetilde{P}) - (1-P)log(1-\widetilde{P})$

​	仍然符合上述一般形式。



​	本文是基于知乎专栏文章《一文搞懂熵(Entropy),交叉熵(Cross-Entropy)》和书籍《神经网络与深度学习》相关章节的读书笔记。

​	reference：

​	https://zhuanlan.zhihu.com/p/149186719

​	https://nndl.github.io/

